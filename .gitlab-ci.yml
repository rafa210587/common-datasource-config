#
# Auto DevOps
#
# Este arquivo customizado do CI/CD do Gitlab, executa em ordem:
#
# 1 - Analise o código fonte dos arquivos utilizando o SonarQube
# 2 -  Identifica Dockerfiles no repositório e faz a build e push (opcional) da imagem para o Registry do Gitlab (ou utiliza o Heroku para buildar a imagem baseado na linguagem utilizada);
# 3 - Faz o deploy no Kubernetes em staging (se branch=master) ou review (se branch!=master);
# 4.1 -  Executa testes em staging ou review usando o Cucumber no Firefox e no Chrome;
# 4.2 - Faz a análise da imagem no Aqua;
# 5 - Faz o deploy em production (step manual, caso branch=master);
# 6 - Executa testes em production usando o Cucumber no Firefox e no Chrome;
# 7 - APENAS BRANCHES: Remove o deployment de review (step manual);
#
# O projeto deve estar configurado com integração com Kubernetes
# A variável AUTO_DEVOPS_DOMAIN é obrigatória e deve ser definido em deploy/config.conf
#
# Use um . antes do nome de qualquer job para omiti-lo durante o pipeline
#
# Caso a build falhe em detectar um buildpack, é possível fornecer um na variável
# `BUILDPACK_URL` ex: BUILDPACK_URL=https://github.com/heroku/heroku-buildpack-ruby.git#v142
# Se forem necessários multiplos buildpacks, adicionar ao projeto um arquivo '.buildpacks'
# e coloque uma URL por linha.
# Nota: Multiplos buildpacks não são suportados atualmente
#
#


image: alpine:latest

variables:
  STAGING_REPLICAS: 2
  PRODUCTION_REPLICAS: 3
  GIT_SSL_NO_VERIFY: $GIT_SSL_NO_VERIFY_CONF

stages:
  - Code_Analysis
  - build
  - review
  - code_analysis_review
  - Deploy-Staging
  - Deploy-Developer
  - Tests-Staging
  - canary
  - Gate-Production
  - Deploy-Production
  - Tests-Production
  - cleanup

# Job de análise on-demand da aplicação no SonarQube em review. Não faz o upload dos dados para a plataforma
.review-code-quality-sonarqube-preview:
  image: $CI_REGISTRY/agilitystack-helper/images/master-sonar:latest
  allow_failure: false
  stage: Tests-Staging
  script:
  - (if [[ -f pom.xml ]]; then mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=preview; else echo "<project><modelVersion>4.0.0</modelVersion><groupId>agility.cloud</groupId><artifactId>$CI_PROJECT_PATH_SLUG</artifactId><version>$CI_PIPELINE_ID-$CI_JOB_ID</version></project>" > pom.xml && mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=preview -Dsonar.gitlab.commit_sha=$CI_BUILD_REF -Dsonar.gitlab.ref_name=$CI_BUILD_REF_NAME -Dsonar.gitlab.project_id=$CI_PROJECT_ID -Dsonar.sources=.; fi);
  except:
  - master
  
# Job de análise on-demand da aplicação no SonarQube em staging. Não faz o upload dos dados para a plataforma
.code-quality-sonarqube-preview:
  image: $CI_REGISTRY/agilitystack-helper/images/master-sonar:latest
  allow_failure: false
  stage: Code_Analysis
  script:
  - (if [[ -f pom.xml ]]; then mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=preview; else echo "<project><modelVersion>4.0.0</modelVersion><groupId>agility.cloud</groupId><artifactId>$CI_PROJECT_PATH_SLUG</artifactId><version>$CI_PIPELINE_ID-$CI_JOB_ID</version></project>" > pom.xml && mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=preview -Dsonar.gitlab.commit_sha=$CI_BUILD_REF -Dsonar.gitlab.ref_name=$CI_BUILD_REF_NAME -Dsonar.gitlab.project_id=$CI_PROJECT_ID -Dsonar.sources=.; fi);
  only:
  - master
  
# Job de análise on-demand da aplicação no SonarQube em staging. Faz o upload dos dados para a plataforma (executado só na master)
.code-quality-sonarqube-publish:
  image: $CI_REGISTRY/agilitystack-helper/images/master-sonar:latest
  allow_failure: false
  stage: Code_Analysis
  script:
  - (if [[ -f pom.xml ]]; then mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=publish; else echo "<project><modelVersion>4.0.0</modelVersion><groupId>agility.cloud</groupId><artifactId>$CI_PROJECT_PATH_SLUG</artifactId><version>$CI_PIPELINE_ID-$CI_JOB_ID</version></project>" > pom.xml && mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=publish -Dsonar.sources=.; fi);
  - wget https://github.com/gabrie-allaigre/sonar-gate-breaker/releases/download/1.0.1/sonar-gate-breaker-all-1.0.1.jar
  - java -jar sonar-gate-breaker-all-1.0.1.jar -u $SONAR_LOGIN
  only:
  - master
  
# Job para build da aplicação.
build:
  stage: build
  image: docker:git
  services:
  - docker:dind
  variables:
    DOCKER_DRIVER: overlay2
  script:
    - setup_docker
    - build
  only:
    - branches

.Aprova-GMUD-Production:
 stage: Gate-Production
 when: manual
 allow_failure: false
 script:
  - echo "Aprovado por $GITLAB_USER_NAME - $GITLAB_USER_EMAIL"!
 only:
 - master

.production-ansible:
 stage: Deploy-Production
 retry: 2
 allow_failure: false
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-alpine-ansible
 script:
  - (if [[ -f deploy/ansible/playbook.yml ]]; then ansible-playbook deploy/ansible/playbook.yml -vvvv -i deploy/ansible/hosts.yml -l production; else echo "Nenhum Playbook Ansible encontrado, prosseguindo..."; fi);
 only:
 - master

.staging-ansible:
 stage: Deploy-Staging
 retry: 2
 allow_failure: false
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-alpine-ansible
 script:
  - (if [[ -f deploy/ansible/playbook.yml ]]; then ansible-playbook deploy/ansible/playbook.yml -vvvv -i deploy/ansible/hosts.yml -l staging; else echo "Nenhum Playbook Ansible encontrado, prosseguindo..."; fi);
 environment:
    name: staging
    url: http://$CI_PROJECT_PATH_SLUG-staging.$AUTO_DEVOPS_DOMAIN
 only:
 - master

# Job para deploy da aplicação em review (apenas branch!=master) 
.review:
  stage: review
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy
    - echo export CI_ENVIRONMENT_SLUG=$CI_ENVIRONMENT_SLUG >> variables
  environment:
    name: review/$CI_COMMIT_REF_NAME
    url: http://$CI_PROJECT_PATH_SLUG-$CI_ENVIRONMENT_SLUG.$AUTO_DEVOPS_DOMAIN
    on_stop: stop_review
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master
  artifacts:
    paths:
    - variables

# Job manual para remoção do deploy da aplicação em review (apenas branch!=master)
.stop_review:
  stage: cleanup
  variables:
    GIT_STRATEGY: none
  script:
    - install_dependencies
    - delete
  environment:
    name: review/$CI_COMMIT_REF_NAME
    action: stop
  when: manual
  allow_failure: true
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master

# Job para deploy da aplicação em area de staging (apenas branch=master)
.staging-helm:
  stage: Deploy-Staging
  retry: 2
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy
  environment:
    name: staging
    url: http://$CI_PROJECT_PATH_SLUG-staging.$AUTO_DEVOPS_DOMAIN
  only:
    refs:
      - master
    kubernetes: active

# Job para deploy da aplicação em area de staging (apenas branch=master)
.developer-helm:
  stage: Deploy-Developer
  retry: 2
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy
  environment:
    name: developer
    url: http://$CI_PROJECT_PATH_SLUG-developer.$AUTO_DEVOPS_DOMAIN
  only:
    refs:
      - branches
    kubernetes: active
  except:
    - master


# Job para testes da aplicação em staging utilizando o Cucumber com Chrome WebDriver (apenas branch=master)	
.selenium-test-integrated-staging-cucumber-chrome:
 stage: Tests-Staging
 allow_failure: false
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-cucumber-ruby
 services:
  - name: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-selenium-standalone-chrome
 script:
  - curl -s --connect-timeout 10 --max-time 20 --retry 10 --retry-delay 0 --retry-max-time 200 --retry-connrefused http://localhost:4444/wd/hub/ #wait until selenium is working
  - cp -R ./deploy/* .
  - cucumbertest staging chrome
 artifacts:
  paths:
   - ./resultados_chrome.html
  when: always
 only:
 - master


# Job para testes da aplicação em staging utilizando o Cucumber com Firefox (GeckoWebDriver) (apenas branch=master)	
.selenium-test-integrated_staging_cucumber-firefox:
 stage: Tests-Staging
 allow_failure: false
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-cucumber-ruby 
 services:
  - name: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-selenium-standalone-firefox
 script:
 - curl -s --connect-timeout 10 --max-time 20 --retry 10 --retry-delay 0 --retry-max-time 200 --retry-connrefused http://localhost:4444/wd/hub/ #wait until selenium is working
 - cp -R ./deploy/* .
 - cucumbertest staging ff
 artifacts:
  paths:
   - ./resultados_firefox.html
  when: always
 only:
 - master

# Job para testes da aplicação em reviews utilizando o Cucumber com Chrome WebDriver (apenas branch!=master)	
.selenium_cucumber_integrated_tests_review_chrome:
 stage: Tests-Staging
 allow_failure: true
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-cucumber-ruby
 services:
  - name: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-selenium-standalone-chrome
 script:
  - curl -s --connect-timeout 10 --max-time 20 --retry 10 --retry-delay 0 --retry-max-time 200 --retry-connrefused http://localhost:4444/wd/hub/ #wait until selenium is working
  - cucumbertest review chrome
 artifacts:
  paths:
   - ./resultados_chrome.html
  when: always
 only:
    refs:
      - branches
    kubernetes: active
 except:
   - master

   
# Job para testes da aplicação em reviews utilizando o Cucumber com Firefox (GeckoWebDriver) (apenas branch!=master).	   
.selenium_cucumber_integrated_tests_review_firefox:
 stage: Tests-Staging
 allow_failure: true
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-cucumber-ruby
 services:
  - name: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-selenium-standalone-firefox
 script:
  - source variables
  - curl -s --connect-timeout 10 --max-time 20 --retry 10 --retry-delay 0 --retry-max-time 200 --retry-connrefused http://localhost:4444/wd/hub/ #wait until selenium is working
  - cucumbertest review ff
 artifacts:
  paths:
   - ./resultados_firefox.html
  when: always
 only:
    refs:
      - branches
    kubernetes: active
 except:
   - master

# Job para execução de testes da imagem criada via Aqua Container Security (apenas branch=master).   
.security-image-aqua:
  stage: Tests-Staging
  allow_failure: true
  image: docker:git
  services:
  - docker:dind
  variables:
    DOCKER_DRIVER: overlay2
  script:
    - setup_docker
    - docker login -u $AQUA_DOCKER_USER -p $AQUA_DOCKER_PASS
    - docker run --rm -v /$CI_PROJECT_PATH:/tmp -v /var/run/docker.sock:/var/run/docker.sock aquasec/scanner-cli:3.0 scan --scan-malware -H $AQUA_URL -U $AQUA_SCAN_USER -P $AQUA_SCAN_PASS --registry $AQUA_SCAN_REGISTRY $CI_PROJECT_PATH/$CI_BUILD_REF_SLUG --register --htmlfile /tmp/aquascan-results.html
  artifacts:
    paths:
    - aquascan-results.html
    when: always
  only:
  - master


# Deployments Canary estão desabilitados e aqui mantidos apenas para uso futuro
.canary:
  stage: canary
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy canary
  environment:
    name: production
    url: http://$CI_PROJECT_PATH_SLUG.$AUTO_DEVOPS_DOMAIN
  when: manual
  only:
    refs:
      - master
    kubernetes: active


# Job manual para criação do deployment em production, após a execução de todos os testes.
.production-helm:
  stage: Deploy-Production
  retry: 2
  allow_failure: false
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - create_secret
    - deploy
    - delete canary
  environment:
    name: production
    url: http://$CI_PROJECT_PATH_SLUG.$AUTO_DEVOPS_DOMAIN
  when: manual
  only:
    refs:
      - master
    kubernetes: active

# Job para testes da aplicação em production utilizando o Cucumber com Chrome WebDriver (apenas branch=master)	
.selenium-test-integrated-production-cucumber-chrome:
 stage: Tests-Production
 allow_failure: false
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-cucumber-ruby
 services:
  - name: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-selenium-standalone-chrome
 script:
  - curl -s --connect-timeout 10 --max-time 20 --retry 10 --retry-delay 0 --retry-max-time 200 --retry-connrefused http://localhost:4444/wd/hub/ #wait until selenium is working
  - cucumbertest production chrome
 artifacts:
  paths:
   - ./resultados_chrome.html
  when: always
 only:
 - master


# Job para testes da aplicação em production utilizando o Firefox (GeckoWebDriver) (apenas branch=master)	
.selenium-test-integrated-production-cucumber-firefox:
 stage: Tests-Production
 allow_failure: false
 image: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-cucumber-ruby
 services:
  - name: registry.$AUTO_DEVOPS_DOMAIN/agilitystack-helper/images/master-selenium-standalone-firefox
 script:
  - curl -s --connect-timeout 10 --max-time 20 --retry 10 --retry-delay 0 --retry-max-time 200 --retry-connrefused http://localhost:4444/wd/hub/ #wait until selenium is working
  - cucumbertest production ff
 artifacts:
  paths:
   - ./resultados_firefox.html
  when: always
 only:
 - master

# ---------------------------------------------------------------------------
# Abaixo estão todas as custom functions necessárias para executar as funções de deployment. 
# O conteúdo abaixo é integralmente fornecido ao contexto do Gitlab Runner durante a execução dos jobs
# Caso seja necessário utilizar qualquer function em um job customizado, basta referenciá-la
# pelo nome na diretiva "script" dentro do job.
#
# Novas funções podem ser adicionadas, respeitando o inicio/fim de funções existentes
#


.auto_devops: &auto_devops |
  # Auto DevOps variables and functions
  [[ "$TRACE" ]] && set -x
  auto_database_url=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${CI_ENVIRONMENT_SLUG}-postgres:5432/${POSTGRES_DB}
  export DATABASE_URL=${DATABASE_URL-$auto_database_url}
  export CI_APPLICATION_REPOSITORY=$CI_REGISTRY_IMAGE/$CI_COMMIT_REF_SLUG
  export CI_APPLICATION_TAG=$CI_COMMIT_SHA
  export CI_CONTAINER_NAME=ci_job_build_${CI_JOB_ID}
  export TILLER_NAMESPACE=$KUBE_NAMESPACE
 

  function deploy() {
    source ./deploy/config.conf  
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    replicas="1"
    service_enabled="false"
    postgres_enabled="$POSTGRES_ENABLED"
    # canary uses stable db
    [[ "$track" == "canary" ]] && postgres_enabled="false"

    env_track=$( echo $track | tr -s  '[:lower:]'  '[:upper:]' )
    env_slug=$( echo ${CI_ENVIRONMENT_SLUG//-/_} | tr -s  '[:lower:]'  '[:upper:]' )
    
    if echo "$CI_ENVIRONMENT_SLUG" | egrep "review|staging"; then
      env_selector='ENV-DESENVOLVIMENTO'
    else
      env_selector='ENV-PRODUCAO'
    fi

    if [[ "$track" == "stable" ]]; then
      # for stable track get number of replicas from `PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_slug}_REPLICAS
      service_enabled="true"
    else
      # for all tracks get number of replicas from `CANARY-production_REPLICAS`
      eval new_replicas=\$${env_track}_${env_slug}_REPLICAS
    fi
    if [[ -n "$new_replicas" ]]; then
      replicas="$new_replicas"
    fi

    # WORKAROUND - Ajuste para problemas com RBAC. Referência: https://gitlab.com/charts/charts.gitlab.io/issues/118 e https://gitlab.com/gitlab-org/gitlab-ce/issues/44597
    kubectl describe clusterrolebinding $KUBE_NAMESPACE-cluster-rule || kubectl create clusterrolebinding $KUBE_NAMESPACE-cluster-rule --clusterrole=cluster-admin --serviceaccount=$KUBE_NAMESPACE:default

    helm upgrade --install \
      --wait \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set application.track="$track" \
      --set application.database_url="$DATABASE_URL" \
      --set service.url="$CI_ENVIRONMENT_URL" \
      --set service.build_name="$CI_BUILD_NAME" \
      --set replicaCount="$replicas" \
      --set service.healthcheck.endpoint="$SERVICE_HEALTHCHECK_ENDPOINT" \
      --set postgresql.enabled="$postgres_enabled" \
      --set postgresql.nameOverride="postgres" \
      --set postgresql.postgresUser="$POSTGRES_USER" \
      --set postgresql.postgresPassword="$POSTGRES_PASSWORD" \
      --set postgresql.postgresDatabase="$POSTGRES_DB" \
      --set application.environment="$env_selector" \
      --set livenessprobe.initialDelaySeconds="$LIVENESS_PROBE_INITIAL_DELAY_SEC" \
      --set readinessprobe.initialDelaySeconds="$READINESS_PROBE_INITIAL_DELAY_SEC" \
      --set persistence.enabled="$PERSISTENCE_ENABLED" \
      --set persistence.size="$PERSISTENCE_SIZE" \
      --set persistence.path="$PERSISTENCE_PATH" \
      --set service.probe.healthcheck="$HEALTH_CHECK_ENDPOINT" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      "$name" \
      chart/
            
    kubectl rollout status -n "$KUBE_NAMESPACE" -w "deployment/$CI_ENVIRONMENT_SLUG"
    
  }

  function install_dependencies() {
    apk add -U openssl curl tar gzip bash ca-certificates git
    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-glibc/master/sgerrand.rsa.pub
    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.23-r3/glibc-2.23-r3.apk
    apk add glibc-2.23-r3.apk
    rm glibc-2.23-r3.apk

    curl https://kubernetes-helm.storage.googleapis.com/helm-v2.6.1-linux-amd64.tar.gz | tar zx
    mv linux-amd64/helm /usr/bin/
    helm version --client
    curl -L -o /usr/bin/kubectl  https://storage.googleapis.com/kubernetes-release/release/v1.9.6/bin/linux/amd64/kubectl

    chmod +x /usr/bin/kubectl
    kubectl version --client
  }

  function setup_docker() {
    if ! docker info &>/dev/null; then
      if [ -z "$DOCKER_HOST" -a "$KUBERNETES_PORT" ]; then
        export DOCKER_HOST='tcp://localhost:2375'
      fi
    fi
  }

  function setup_test_db() {
    if [ -z ${KUBERNETES_PORT+x} ]; then
      DB_HOST=postgres
    else
      DB_HOST=localhost
    fi
    export DATABASE_URL="postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:5432/${POSTGRES_DB}"
  }

  function download_chart() {
    # if [[ ! -d chart ]]; then
    helm init --client-only
    git clone ${CHART_REPO_URL} chart
    helm dependency update chart/
    helm dependency build chart/
  }


  function ensure_namespace() {
        kubectl describe namespace "$KUBE_NAMESPACE" || kubectl create namespace "$KUBE_NAMESPACE"
  }

  function check_kube_domain() {
    if [ -z ${AUTO_DEVOPS_DOMAIN+x} ]; then
      echo "Domínio para deploy não definido. Um domínio deve ser definido na variável AUTO_DEVOPS_DOMAIN dentro de deploy/config.conf"
      false
    else
      true
    fi
  }

  function build() {
    if [[ -n "$CI_REGISTRY_USER" ]]; then
      echo "Autenticação no Gitlab Container Registry com as credenciais de CI.."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" "$CI_REGISTRY"
      echo ""
    fi
   
    source ./deploy/config.conf
    
    if [[ -d deploy/services/ ]]; then
      cd deploy/services/
      for f in *; do
          if [[ -d $f ]]; then
          # $f is a directory
          echo "Build do Dockerfile em $f..."
          if [ "$f" != "default" ]; then echo "pasta diferente de default!" ;SERVICE_DIR_NAME="-$f"; fi
            if [ -f $f/config.conf ]; then source $f/config.conf; fi 
            docker build -t "$CI_APPLICATION_REPOSITORY$SERVICE_DIR_NAME:$CI_APPLICATION_TAG" -f $f/Dockerfile ../../
          fi  
          if [[ $IMAGE_PUSH != "false" ]]; then
            echo "Nova imagem com tag latest criada... armazenando noo Gitlab Registry"
            docker tag "$CI_APPLICATION_REPOSITORY$SERVICE_DIR_NAME:$CI_APPLICATION_TAG" "$CI_APPLICATION_REPOSITORY$SERVICE_DIR_NAME:latest"
            echo "Pushing to GitLab Container Registry -$SERVICE_DIR_NAME ..."
            docker push "$CI_APPLICATION_REPOSITORY$SERVICE_DIR_NAME:$CI_APPLICATION_TAG"
            docker push "$CI_APPLICATION_REPOSITORY$SERVICE_DIR_NAME:latest"
          fi
      done
    else
      echo "Building Heroku-based application using gliderlabs/herokuish docker image..."
      docker run -i --name="$CI_CONTAINER_NAME" -v "$(pwd):/tmp/app:ro" gliderlabs/herokuish /bin/herokuish buildpack build
      docker commit "$CI_CONTAINER_NAME" "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG"
      docker rm "$CI_CONTAINER_NAME" >/dev/null
      echo ""
      echo "Configuring $CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG docker image..."
      docker create --expose 5000 --env PORT=5000 --name="$CI_CONTAINER_NAME" "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG" /bin/herokuish procfile start web
      docker commit "$CI_CONTAINER_NAME" "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG"
      docker rm "$CI_CONTAINER_NAME" >/dev/null
    fi
    }

  function install_tiller() {
    echo "Checking Tiller..."
    helm init --upgrade
    kubectl rollout status -n "$TILLER_NAMESPACE" -w "deployment/tiller-deploy"
    if ! helm version --debug; then
      echo "Failed to init Tiller."
      return 1
    fi
    echo ""
  }

  function create_secret() {
    echo "Create secret..."
    if [[ "$CI_PROJECT_VISIBILITY" == "public" ]]; then
      return
    fi
    
   # Cria secret para primeiro deployment via HELM. Esta secret só funciona no primeiro deploy devido a natureza do gitlab-ci-token. Deploys subsequentes utiulizarão outra secret válida para o usuário registryuser (criada por um cronjob)
   if ( ! $(kubectl get secret -n $KUBE_NAMESPACE gitlab-registry) || $(kubectl get secret gitlab-registry -n $KUBE_NAMESPACE --output="jsonpath={.data.\.dockerconfigjson}" | base64 -d | grep gitlab-ci-token)  ); then
    echo "Criando Secret..."
    kubectl create secret -n "$KUBE_NAMESPACE" \
      docker-registry gitlab-registry \
      --docker-server="$CI_REGISTRY" \
      --docker-username="$CI_REGISTRY_USER" \
      --docker-password="$CI_REGISTRY_PASSWORD" \
      --docker-email="$GITLAB_USER_EMAIL" \
      -o yaml --dry-run | kubectl replace -n "$KUBE_NAMESPACE" --force -f -
     fi
  }

  function delete() {
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    helm delete "$name" || true
  }
  
  function cucumbertest () {
  deployment=$1
  browser=$2
  browserName='firefox'
  LINK=""
  tag=""
  
  cp -R deploy/* .
  cp -R /cucumber/* features/
  
  if [ $deployment = "production" ]; then
  LINK="http://$CI_PROJECT_PATH_SLUG.$AUTO_DEVOPS_DOMAIN"
  tag="--tags @production"
  elif [ $deployment = "staging" ]; then
  LINK="http://$CI_PROJECT_PATH_SLUG-staging.$AUTO_DEVOPS_DOMAIN"
  tag=""
  elif [ $deployment = "review" ]; then
  LINK="http://$CI_PROJECT_PATH_SLUG-$CI_ENVIRONMENT_SLUG.$AUTO_DEVOPS_DOMAIN"
  tag="--tags @review"
  else
  echo "Nenhum ambiente foi fornecido. Abortando"
  return 1
  fi
  
  if [ $browser = "ff" ]; then
     browserName="firefox" 
  else
     browserName="chrome"
  fi
  

  if cucumber -c $tag URL=$LINK -f json -o output-chrome.json -f pretty BROWSER=$browser; then
    echo "Todos os testes apresentaram sucesso! Gerando relatório..." 
    report_builder -s output-chrome.json --images -f html -T "Relatorio de Testes no $browserName - Cucumber" -o resultados_$browserName
    return 0

  #se algum teste falhar, efetuar o bypass da falha, gerar o relatório e falhar a task    
  else
    echo "Um ou mais testes apresentaram falha. Gerando relatório..."
    report_builder -s output-chrome.json --images -f html -T "Relatorio de Testes no $browserName - Cucumber" -o resultados_$browserName
    return 1
  fi
  }

before_script:
  - source ./deploy/config.conf
  - *auto_devops
  

